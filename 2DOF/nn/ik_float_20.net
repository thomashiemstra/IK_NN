FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000001490116120000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=3 21 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) (21, 0, 1.00000000000000000000e+000) (21, 0, 1.00000000000000000000e+000) (0, 0, 1.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.47094748373855900000e+000) (1, 1.98443279005841800000e+000) (2, -4.21570841169289280000e-001) (0, -2.02782794466390160000e-001) (1, 4.11939955916585440000e-001) (2, -5.33469731485265000000e-001) (0, -1.99824822072208090000e+000) (1, 2.38537005070568810000e+000) (2, 2.66651498226323720000e-001) (0, 5.19201888421788650000e+000) (1, 2.34274029570089540000e+000) (2, 1.94697781589207140000e+000) (0, 4.47746020299592830000e+000) (1, 5.98121323337388500000e+000) (2, 5.90113041527430050000e+000) (0, 4.69201951881836710000e+000) (1, 2.04815092685598990000e+000) (2, 3.75304909414424510000e+000) (0, 8.22708323481684100000e-001) (1, 2.99685774929209850000e+000) (2, -1.46081711178897770000e+000) (0, 1.31352649340484470000e+000) (1, 7.89112340383748110000e-001) (2, 1.13935309260230280000e+000) (0, 8.88385750957234490000e-001) (1, -1.82859592427700330000e+000) (2, 1.71830630718096170000e+000) (0, -1.08217582816132430000e+000) (1, 3.88686408754647820000e+000) (2, 6.40332114710571830000e-001) (0, 4.24701656393317780000e+000) (1, 1.64118251166059980000e+000) (2, 1.77607828719690360000e+000) (0, 1.49996221346152150000e+003) (1, 1.50000000000000000000e+003) (2, 1.50000000000000000000e+003) (0, 9.56687978409255370000e-002) (1, -2.52505357036746590000e+000) (2, 3.30773444254983500000e+000) (0, 3.05052090167677600000e+000) (1, 8.59109405478484240000e-001) (2, 1.35761873916687700000e+000) (0, 2.18163090298025650000e+000) (1, -2.02314754060708530000e-001) (2, 2.28402663264448560000e+000) (0, -1.93103138864644700000e+000) (1, -5.73734976005675690000e-001) (2, 2.67280978883939160000e+000) (0, 5.55580647101476810000e-001) (1, -7.54879240504871100000e-001) (2, -5.52861640003178390000e-001) (0, -3.30461291359812130000e+000) (1, 2.62295261262976620000e+000) (2, 1.02034639837852860000e+000) (0, 2.47929726351231580000e+000) (1, 2.04579105320512780000e+000) (2, 1.90132616380056850000e+000) (0, 3.83451926818156120000e+000) (1, 2.07823301205553390000e+000) (2, 1.26003369240011650000e+000) (3, -8.99940337479132270000e-001) (4, 6.50506739823152990000e+000) (5, 2.41299935026526400000e+000) (6, 3.20539963072755340000e-001) (7, -3.45235461030455270000e-001) (8, -6.99295610758689400000e-001) (9, -2.22204719508295870000e-001) (10, 1.26451544744264250000e+000) (11, -7.52353641639880280000e-001) (12, 1.45289219883467390000e+000) (13, 1.39596373148015250000e+000) (14, -9.00495169729099850000e-002) (15, 3.40924489264804540000e+000) (16, 5.16987994829574140000e-001) (17, 2.30382646909835430000e+000) (18, 2.31982108226952690000e+000) (19, 7.96439558218458200000e+000) (20, -1.05497404956325820000e+000) (21, -5.85080850926294920000e-001) (22, -2.36466810233285110000e+000) (23, -1.92279500620903600000e-001) (3, -9.82291147430874200000e-001) (4, 7.68275269314594580000e+000) (5, 2.28071242526415950000e-001) (6, 9.06900799153107730000e-001) (7, -1.51506825632498930000e+000) (8, -1.61510622400654440000e+000) (9, 1.74180484086066210000e-001) (10, -5.43418983775223390000e-002) (11, 1.29433665388586920000e-001) (12, 1.12284012423424870000e+000) (13, -1.34147042595198780000e+000) (14, -1.94228791221424890000e+000) (15, 4.41333142761245510000e+000) (16, 1.35037164137091610000e+000) (17, 4.29366049640588000000e+000) (18, 4.41166361292189000000e+000) (19, 7.09507508033174260000e+000) (20, 1.32138567229830420000e-001) (21, 1.25671645606195190000e+000) (22, -1.11705211296228100000e+000) (23, -1.79595910362033990000e+000) 
