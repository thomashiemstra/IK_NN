FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000001490116120000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=3 21 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) (21, 0, 1.00000000000000000000e+000) (21, 0, 1.00000000000000000000e+000) (0, 0, 1.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.81519953211265280000e+000) (1, 8.87208408773155990000e-001) (2, 2.38349467270034230000e+000) (0, -4.16330656439698860000e-003) (1, -1.17823997497599690000e+000) (2, 2.05622120161244700000e+000) (0, -1.07956438343775730000e+000) (1, 3.07468702326197830000e-001) (2, 1.82166266621676760000e+000) (0, -2.22258941221059600000e+000) (1, 1.49999411039957570000e+003) (2, 4.96963249068695220000e+000) (0, 1.34657746785600930000e+001) (1, 1.48077719953587050000e+003) (2, 1.29813520469515980000e+001) (0, 4.27006776863210740000e+000) (1, 1.58171776491243140000e+000) (2, 5.20563486688783430000e+000) (0, -2.91314077980620030000e-001) (1, -1.09294501359121180000e+000) (2, -8.13966486317076130000e-001) (0, 5.60645145867576210000e+000) (1, 6.59594696364421470000e-001) (2, 1.35940169849682690000e+001) (0, -1.89236214771348250000e-001) (1, 1.92070882884628650000e-001) (2, -1.36750162781109700000e+000) (0, 1.49725716242756470000e+003) (1, 1.49722181997035550000e+003) (2, 1.49898291751908660000e+003) (0, 7.80336518816393540000e+000) (1, 1.58282127953764860000e+000) (2, 1.04305597695875290000e+001) (0, -3.59378548727942170000e-001) (1, 9.23864491932202330000e-001) (2, 1.26125791018595090000e+000) (0, 6.34292638417243700000e-001) (1, 2.39128249110909690000e+000) (2, 8.53187714056748090000e-001) (0, 1.49660019014203390000e+003) (1, 4.13266201764587830000e+002) (2, 1.47712406247870560000e+003) (0, -7.14274721146577970000e-001) (1, 3.57941845289520180000e+000) (2, 2.83247346543248570000e+000) (0, 6.64279785530753730000e-001) (1, 5.97195924710414380000e-001) (2, -5.40266348116939660000e-002) (0, 1.29621249926245780000e+002) (1, 1.49923889865043840000e+003) (2, 1.31704396014695660000e+001) (0, -1.49498193483082920000e+003) (1, -1.30565474984956500000e+002) (2, 1.49966265737345250000e+003) (0, 8.79520135885188850000e-001) (1, -7.56868057664125900000e-001) (2, -1.97504147011043750000e+000) (0, -3.84926211040597880000e-001) (1, -2.51998198266830840000e-001) (2, -6.33806097260960490000e-001) (3, 4.01078229192150190000e+000) (4, 1.36184939201867950000e+001) (5, 9.43505793284604620000e+000) (6, 1.12224418785927170000e+001) (7, 6.43686912996520810000e+000) (8, 3.13598806299802440000e+000) (9, 5.06005145865874580000e+001) (10, 8.19381771230040810000e+000) (11, 5.54217788667233080000e+001) (12, 5.87129511706623750000e+000) (13, 4.08777118713300070000e+000) (14, 3.97311609074408520000e+001) (15, 1.39022586422872470000e+001) (16, 5.74147487055817950000e+000) (17, 1.71009935322621520000e+001) (18, 2.08059696903558340000e+001) (19, 5.07854765999208270000e+000) (20, 4.75742012839446100000e+000) (21, 4.52245406641529540000e+001) (22, 3.25981301922835060000e+001) (23, 4.94995942124770850000e+000) (3, 4.65799844511330360000e+000) (4, 2.08997175795801550000e+001) (5, 1.53616877160819920000e+001) (6, 8.64401533631003890000e+000) (7, 1.14213640982546650000e+001) (8, 7.48588412526771130000e+000) (9, 5.36801292114458240000e+001) (10, 7.26605875588807940000e+000) (11, 6.51996361250725160000e+001) (12, 5.23265964266253200000e+000) (13, 5.71024636158093470000e+000) (14, 3.34262243879388810000e+001) (15, 1.21031955969511990000e+001) (16, 4.79418535925991840000e+000) (17, 1.49489163276319670000e+001) (18, 2.45778538669248300000e+001) (19, 7.76466479721692160000e+000) (20, 4.57707463355926600000e+000) (21, 4.41187703087833470000e+001) (22, 3.39676014076523100000e+001) (23, 3.15564593339353870000e+000) 
