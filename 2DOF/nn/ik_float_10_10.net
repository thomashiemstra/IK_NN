FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000001490116120000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=3 11 11 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) (11, 0, 1.00000000000000000000e+000) (11, 0, 1.00000000000000000000e+000) (0, 0, 1.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.39049828854085800000e-001) (1, -5.58574892897637840000e-001) (2, -2.41700741577130370000e-001) (0, 9.36128792034669920000e+000) (1, 3.12911393345332240000e+000) (2, 2.71520821651707190000e+000) (0, 7.32430005915363400000e+000) (1, 2.00882842612317210000e+000) (2, 1.17892007992619700000e+000) (0, 1.13038846124410420000e+000) (1, 8.02536730399730500000e-001) (2, -9.46154001152174630000e-001) (0, -1.39160517535265650000e+003) (1, 1.49941838697791080000e+003) (2, 1.49902331304313590000e+003) (0, 3.54815258491877250000e+000) (1, 1.46218188776910730000e-001) (2, 1.81511363179659800000e-001) (0, -1.32700949253663650000e+000) (1, 1.18419507234558900000e+000) (2, 1.11974850312518550000e+000) (0, 1.47188438854626800000e+000) (1, 4.41108166358235200000e+000) (2, 6.80457307366123200000e-001) (0, -2.20691381564601880000e+001) (1, 1.51228214198506150000e+001) (2, -2.57650485589968380000e+001) (0, 1.46235986883783950000e+003) (1, -3.49397438544646890000e+002) (2, 1.49999419036755490000e+003) (3, 5.74791716748166600000e+000) (4, 1.28437608459553120000e+000) (5, 1.73474082153179540000e+000) (6, 8.08998274045367260000e+000) (7, 2.83008812764949670000e+000) (8, 1.42295099513223990000e+000) (9, -2.23062050849869210000e+000) (10, 3.19190555716267800000e+000) (11, 4.43125380234910940000e-001) (12, 2.63518172873433490000e+000) (13, 2.63178607615801850000e+000) (3, 3.29968075729142550000e+000) (4, 3.17896094719054110000e+000) (5, 2.51385108381241280000e+000) (6, 1.82198315616939130000e+000) (7, -6.32074860491192300000e-002) (8, 1.09334021049362450000e+000) (9, 4.91060161353859750000e+000) (10, -8.11844509897992950000e-001) (11, 3.59125124723065040000e+000) (12, 1.14658627127520310000e+000) (13, -8.45968205670551980000e-002) (3, 1.40476476769488490000e+000) (4, 1.69679761986547520000e-001) (5, 8.69626879866750710000e+000) (6, 1.22855857073210840000e-001) (7, 6.60778423673909380000e-001) (8, 1.28677995588880530000e+000) (9, -1.07328396038396810000e+000) (10, -4.91821010851557890000e+000) (11, 4.46165334063685840000e+000) (12, 1.35048713103261390000e-001) (13, 1.92244272302220380000e-001) (3, 3.38366889193316610000e+000) (4, -5.98009896784481020000e-001) (5, -2.40973764520427910000e+000) (6, 3.01373510871554460000e+000) (7, 8.31905155648771430000e-001) (8, -1.62946603289256480000e+000) (9, 1.66516545376558240000e+000) (10, 1.03043775047539790000e+000) (11, 2.57736056431380560000e+000) (12, 7.76706050524373580000e-001) (13, 4.98848484703503570000e-001) (3, 9.74778096785687430000e+000) (4, 5.16201932836813260000e-001) (5, 4.93270049686305780000e-001) (6, 6.47963102369015000000e-001) (7, 7.89905050177252210000e-001) (8, 4.43071158939266570000e-001) (9, 1.17194685141597720000e+000) (10, 1.27385513097917770000e+000) (11, 2.46520904105347990000e-001) (12, 1.62772381917985040000e+000) (13, 1.60877587810481120000e+000) (3, 1.58304086705368510000e+001) (4, -1.45932717339678390000e-002) (5, -1.26012115316755080000e+003) (6, 7.05352505541016050000e+000) (7, 3.55812294162753060000e-001) (8, 8.39047925717287240000e+000) (9, -9.90691103901471410000e-001) (10, 1.44791091551373570000e+000) (11, -6.53715011397881370000e+001) (12, 4.54257579445144500000e-001) (13, -2.90473736362950770000e+001) (3, -4.64998067699055540000e+000) (4, -9.77467625105498810000e-002) (5, 1.18674057562586090000e+000) (6, 3.49315632377314730000e+000) (7, -3.11064285958586100000e-001) (8, 3.10496887074834050000e+000) (9, 2.40225401859399930000e+000) (10, 4.13289113359952550000e-001) (11, -2.92980841193858640000e-001) (12, 1.61135971558340410000e+000) (13, 4.61227972789565180000e-001) (3, 7.42399816367162120000e+000) (4, -1.20458650782976410000e-001) (5, -3.19814507431693470000e-002) (6, 1.54778137199037660000e+000) (7, -1.39915402103933580000e-001) (8, 5.86680299136137400000e-001) (9, 1.85716840330584220000e+000) (10, 9.76024248621014450000e-001) (11, -8.89031498798958790000e-002) (12, 4.52977769816550520000e-001) (13, -8.37603057734203850000e-001) (3, 3.46858994439719790000e+000) (4, 2.85223305686768920000e+000) (5, 2.21302126456411730000e+001) (6, 2.17263280200310690000e+001) (7, 1.34633963201989900000e+000) (8, -1.13553612289304000000e+001) (9, 1.27544173119233030000e+001) (10, 1.70421480544963580000e+000) (11, 1.37176423179209870000e+001) (12, 1.60866063867955030000e+000) (13, 1.01195622338384110000e+000) (3, 6.57928493152927230000e-001) (4, -1.18766483138190160000e+000) (5, 1.16232457958694300000e+000) (6, 4.70624514641647720000e+000) (7, 8.15278676466130730000e-001) (8, 2.81579339084541710000e+000) (9, 2.49417684820282660000e+000) (10, 2.43611294073381490000e+000) (11, -7.82629733688941790000e-002) (12, 7.57050866099030830000e-001) (13, 8.91207856724458790000e-001) (14, 2.24538178268198550000e+000) (15, 9.22765328428161840000e-001) (16, -6.86938804304038090000e-001) (17, 4.39089871709432430000e+000) (18, 1.60925279582829870000e+000) (19, 1.21879852256655870000e+000) (20, 8.03226335656361370000e-001) (21, 3.46492138056209550000e+000) (22, 1.35473536403894900000e+000) (23, 1.04306762644092530000e+000) (24, 1.16119863006099000000e+000) (14, 1.82624649901745570000e-001) (15, 2.10596038598338840000e+000) (16, -1.21391147509629110000e-001) (17, 4.45893290180580590000e+000) (18, 3.14750165930571860000e+000) (19, 1.30274070347914320000e+000) (20, 1.80543964902015560000e+000) (21, 6.21566301892491780000e+000) (22, 1.44873187336368690000e+000) (23, 2.29431630061387360000e+000) (24, 1.06654952509885930000e+000) 
