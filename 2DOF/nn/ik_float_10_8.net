FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000001490116120000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=3 11 9 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (3, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (0, 5, 1.00000000000000000000e+000) (9, 0, 1.00000000000000000000e+000) (9, 0, 1.00000000000000000000e+000) (0, 0, 1.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.50000000000000000000e+003) (1, 2.77791805629761310000e+002) (2, -1.36683407023189840000e+000) (0, 1.23590743723805920000e+003) (1, -1.40865837977959180000e+003) (2, 1.49716712893682420000e+003) (0, -1.02790622243584880000e+003) (1, -1.50000000000000000000e+003) (2, 9.67132083314959690000e+002) (0, -8.70042625677625890000e+002) (1, -1.50000000000000000000e+003) (2, 6.54719845499420560000e+002) (0, 1.50000000000000000000e+003) (1, -1.46387391028176670000e+003) (2, 9.50968015483406930000e+001) (0, -1.50000000000000000000e+003) (1, 1.21595814288106200000e+003) (2, -5.22341913186154090000e+001) (0, -1.50000000000000000000e+003) (1, -1.50000000000000000000e+003) (2, 5.05057047527954350000e+002) (0, 1.50000000000000000000e+003) (1, -1.32397817055009730000e+003) (2, -4.87330398034445620000e+000) (0, -1.50000000000000000000e+003) (1, 1.24987003526197600000e+003) (2, -5.71898772858114130000e+001) (0, -1.50000000000000000000e+003) (1, -3.27298167931018440000e+002) (2, 5.40640040643593010000e+002) (3, -1.22679337524036440000e+003) (4, -6.61421773238090170000e+002) (5, -1.24318402793527550000e+003) (6, 5.86648905565164230000e+002) (7, -1.50000000000000000000e+003) (8, 1.12068120818977920000e+003) (9, 2.57962908058171080000e+002) (10, 8.49762624485511760000e+002) (11, 1.49894247876373740000e+003) (12, 1.47587510133143770000e+003) (13, -3.01577252985321820000e+002) (3, 2.09906083680148330000e+002) (4, 1.38525113426708090000e+003) (5, -1.50000000000000000000e+003) (6, -1.50000000000000000000e+003) (7, 1.38521245568352790000e+003) (8, -1.34922093220627950000e+003) (9, -1.48785355562495900000e+003) (10, 1.50000000000000000000e+003) (11, -1.27049776981087480000e+003) (12, -7.61058632346861030000e+002) (13, -1.47485627877252160000e+003) (3, 1.50000000000000000000e+003) (4, -7.24247380496868520000e+001) (5, -1.50000000000000000000e+003) (6, -1.50000000000000000000e+003) (7, 1.01159998278617820000e+003) (8, -9.84544477377467500000e+002) (9, -1.50000000000000000000e+003) (10, 1.50000000000000000000e+003) (11, -1.41259999709129350000e+003) (12, -1.50000000000000000000e+003) (13, 3.61993158198525460000e+002) (3, 1.39079999079704270000e+003) (4, 1.50000000000000000000e+003) (5, -1.37863447613764360000e+003) (6, -1.50000000000000000000e+003) (7, 1.49636941149187830000e+003) (8, -1.46042106266192490000e+003) (9, -1.50000000000000000000e+003) (10, 1.50000000000000000000e+003) (11, -1.36939999194145180000e+003) (12, -1.09755013706683280000e+003) (13, -3.47918354885811250000e+002) (3, -1.23237636985272270000e+003) (4, -1.46522005439365240000e+003) (5, -1.44254017441283800000e+003) (6, 4.20939946870041010000e+002) (7, -1.47680830012705110000e+003) (8, -1.07210242840292580000e+002) (9, 1.37852026407057680000e+003) (10, -1.33683238949625550000e+003) (11, 2.26451248495763110000e+002) (12, 8.66106243132921580000e+002) (13, -7.97061846130246520000e+002) (3, 1.38439776477223130000e+003) (4, 1.49268319728579810000e+003) (5, 1.50000000000000000000e+003) (6, 1.49953089138281440000e+003) (7, 1.28421743517353390000e+003) (8, -6.74116711604710990000e+002) (9, 1.38407019007100080000e+003) (10, 4.25728287058853310000e+002) (11, 2.97290163727622670000e+001) (12, 1.31145664864265790000e+003) (13, -4.70005183481980790000e+002) (3, 1.49999719592027710000e+003) (4, 1.46592443642988450000e+003) (5, -6.13317747786274590000e+002) (6, 1.01999564310162180000e+003) (7, 1.47959576976241030000e+003) (8, -3.77676271752532330000e+002) (9, -1.44819977741145680000e+003) (10, 1.48657040255115750000e+003) (11, -5.25119943053020850000e+002) (12, 8.36630498067338180000e+002) (13, 7.00189249759166960000e+002) (3, 8.10482984175880180000e+000) (4, 1.42974090493029210000e+003) (5, 1.50000000000000000000e+003) (6, 1.50000000000000000000e+003) (7, -1.49903295953515930000e+003) (8, 7.43402315594802420000e+002) (9, 1.48589064554412720000e+003) (10, 9.78175030136547430000e+001) (11, 1.49999127588851500000e+003) (12, 7.33450513518855700000e+002) (13, 1.34456371630924540000e+002) (14, 1.72472835409662180000e-002) (15, -1.43311490292730600000e-001) (16, -4.72084782818507910000e-002) (17, -4.91371572951103540000e-002) (18, 1.72649719275928330000e-002) (19, 1.23327484659227060000e-001) (20, -9.96938671278543580000e-002) (21, 1.53979995006843670000e-001) (22, 2.68575893721018480000e-001) (14, -1.33187454088018100000e-001) (15, -9.09353573186003350000e-002) (16, -3.13567998848792840000e-003) (17, -3.07191062736405650000e-002) (18, -2.56353134342588660000e-001) (19, 3.74529556226590570000e-001) (20, -3.07328958028414770000e-002) (21, 1.42282517636068860000e-001) (22, -1.15468280153957170000e-001) 
